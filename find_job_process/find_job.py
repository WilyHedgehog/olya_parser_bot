import asyncio
from sentence_transformers import SentenceTransformer, util
from db.requests import stopwords_cache
from db.requests import get_all_professions_parser
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

professions_cache: dict[str, any] = {}
professions_embeddings_cache: dict[str, any] = {}
stopwords_cache: set[str] = set()

def count_stop_words(text: str) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
    """
    text_lower = text.lower()
    return sum(1 for stop_word in stopwords_cache if stop_word in text_lower)


async def load_professions():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à–∏:
    professions_cache –∏ professions_embeddings_cache.
    """
    global professions_cache, professions_embeddings_cache

    professions = await get_all_professions_parser()

    # –∫–µ—à —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    professions_cache = {
        p["name"]: {
            "desc": p.get("desc", ""),
            "keywords": p.get("keywords", {}),
        }
        for p in professions
    }

    # –∫–µ—à —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –æ–ø–∏—Å–∞–Ω–∏–π
    professions_embeddings_cache = {
        name: model.encode(data["desc"], convert_to_tensor=True)
        for name, data in professions_cache.items()
    }


def get_profession_embeddings() -> dict[str, any]:
    return professions_embeddings_cache


async def analyze_vacancy(text: str, embedding_weight: float = 0.7) -> dict:
    """
    –ê–Ω–∞–ª–∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏ —á–µ—Ä–µ–∑ –∫—ç—à –ø—Ä–æ—Ñ–µ—Å—Å–∏–π –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
    embedding_weight —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    """
    # --- —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ ---
    stop_count = count_stop_words(text)
    if stop_count >= 3:
        return {"status": "blocked", "reason": f"{stop_count} stop words found"}

    lowered = text.lower()

    # --- –æ—á–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º ---
    keyword_scores = {
        name: sum(weight for kw, weight in data["keywords"].items() if kw in lowered)
        for name, data in professions_cache.items()
    }

    # --- —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º ---
    text_emb = model.encode(text, convert_to_tensor=True)
    embeddings = get_profession_embeddings()
    embedding_scores = {
        name: util.cos_sim(text_emb, prof_emb).item()
        for name, prof_emb in embeddings.items()
    }

    # --- –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ ---
    final_scores = {
        name: keyword_scores[name] + embedding_weight * embedding_scores[name]
        for name in professions_cache
    }

    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    return {"status": "ok", "ranked": ranked}

# === –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ===
async def find_job_func(
    vacancy_text: str, embedding_weight: float = 0.7
):
    result = await analyze_vacancy(
        vacancy_text, embedding_weight=embedding_weight
    )

    if result["status"] == "blocked":
        print(f"üö´ –í–∞–∫–∞–Ω—Å–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ ({result['reason']})")
        return False

    vacancy_professions = [
        (prof, score) for prof, score in result["ranked"] if score > 1.0
    ]

    if not vacancy_professions:
        print("‚ö†Ô∏è –í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∏–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.")
        return False

    print("üîé –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–∏:")
    for prof, score in vacancy_professions:
        print(f"  {prof}: {score:.3f}")

    return vacancy_professions


# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    professions = {
        "SMM-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç": {
            "desc": "–†–∞–±–æ—Ç–∞ —Å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏, –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–æ–µ–∫—Ç–æ–≤.",
            "keywords": {"smm": 2, "—Å–æ—Ü—Å–µ—Ç–∏": 1, "instagram": 1, "facebook": 1},
        },
        "–ö–æ–ø–∏—Ä–∞–π—Ç–µ—Ä": {
            "desc": "–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–∞–π—Ç–æ–≤, –±–ª–æ–≥–æ–≤, —Å–æ—Ü—Å–µ—Ç–µ–π –∏ —Ä–∞—Å—Å—ã–ª–æ–∫.",
            "keywords": {"—Ç–µ–∫—Å—Ç—ã": 2, "–∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥": 2, "—Å—Ç–∞—Ç—å–∏": 1, "–∫–æ–Ω—Ç–µ–Ω—Ç": 1},
        },
    }

    text1 = "–¢—Ä–µ–±—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ SMM –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤ Instagram"
    text2 = "–£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞: –∫–∞–∑–∏–Ω–æ, —Å—Ç–∞–≤–∫–∏ –Ω–∞ —Å–ø–æ—Ä—Ç, –∞–∑–∞—Ä—Ç–Ω—ã–µ –∏–≥—Ä—ã, –ø–∞—Ä–∏"

    asyncio.run(find_job_func(text1, professions))
    asyncio.run(find_job_func(text2, professions))


# --- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –ë–î) ---
STOP_WORDS = ["–ø–æ–º–æ–≥—É", "–∏–∑—É—á—É", "–º–µ–Ω—è –∑–æ–≤—É—Ç", "–≤–æ–∑—å–º—É –Ω–∞ —Å–µ–±—è", "–∏—â—É —Ä–∞–±–æ—Ç—É"]
