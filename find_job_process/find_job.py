import asyncio
from sentence_transformers import SentenceTransformer, util
from db.requests import stopwords_cache
from db.requests import get_all_professions_parser, get_all_stopwords
from db.database import Sessionmaker
from db.models import StopWord
from utils.bot_utils import send_message
from sqlalchemy.future import select
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

professions_cache: dict[str, any] = {}
professions_embeddings_cache: dict[str, any] = {}
stopwords_cache: set[str] = set()
stop_embeddings = set()


STOP_EMBEDDINGS_ADS = [
    "–ü–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ –º–æ–π –∫–∞–Ω–∞–ª",
    "–†–µ–∫–ª–∞–º–∞ ‚Äî –∑–∞–ª–æ–≥ —É—Å–ø–µ—Ö–∞",
    "–ü–∏–∞—Ä –∏ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö",
    "–ü–æ–º–æ–≥—É –≤–∞–º —Ä–∞–∑–≤–∏—Ç—å –±–∏–∑–Ω–µ—Å",
    "–û–∫–∞–∑—ã–≤–∞—é —É—Å–ª—É–≥–∏ –ø–æ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—é",
    "–ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –∞–∫–∫–∞—É–Ω—Ç–æ–≤ Instagram –∏ Telegram",
    "–†–µ–∫–ª–∞–º–∞ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º –∫–∞–Ω–∞–ª–∞—Ö",
    "–°–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ –∏ —Ä–µ–∫–ª–∞–º–∞",
    "–ó–∞—Ä–∞–±–æ—Ç–∞–π 1000 —Ä—É–±–ª–µ–π –≤ –¥–µ–Ω—å",
    "–†–∞–±–æ—Ç–∞ –±–µ–∑ –≤–ª–æ–∂–µ–Ω–∏–π",
    "–î–æ—Ö–æ–¥ –Ω–∞ –∫—Ä–∏–ø—Ç–µ –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è—Ö",
    "–ü—Ä–æ–π–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ –∏ –ø–æ–ª—É—á–∏ –±–æ–Ω—É—Å",
    "–ó–∞—Ä–∞–±–æ—Ç–æ–∫ –±–µ–∑ –æ–ø—ã—Ç–∞ –∏ –∑–Ω–∞–Ω–∏–π",
    "–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ",
    "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫—É—Ä—Å –ø–æ –∑–∞—Ä–∞–±–æ—Ç–∫—É",
]

STOP_EMBEDDINGS_RESUME = [
    "–ì–æ—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ",
    "–†–∞—Å—Å–º–æ—Ç—Ä—é –ª—é–±—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
    "–ú–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–æ –Ω–∏–∂–µ",
    "–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã 3 –≥–æ–¥–∞",
    "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–Ω–Ω–∞, —è –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥",
    "–ì–æ—Ç–æ–≤ –∫ –ø–µ—Ä–µ–µ–∑–¥—É –∏ –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞–º",
    "–ò—â—É —Å—Ç–∞–∂–∏—Ä–æ–≤–∫—É",
    "–ú–æ–π –æ–ø—ã—Ç –≤ IT –±–æ–ª—å—à–µ 5 –ª–µ—Ç",
    "–•–æ—á—É —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –≤ —ç—Ç–æ–π —Å—Ñ–µ—Ä–µ",
    "–†–∞—Å—Å–º–æ—Ç—Ä—é –æ—Ñ—Ñ–µ—Ä—ã",
    "–ü–∏—à–∏—Ç–µ, –µ—Å–ª–∏ –∏—â–µ—Ç–µ –¥–∏–∑–∞–π–Ω–µ—Ä–∞",
    "–ì–æ—Ç–æ–≤ –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –∑–∞–¥–∞–Ω–∏—é",
]


STOP_EMBEDDINGS_SCAM = [
    "–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥ –±–µ–∑ —Ä–∏—Å–∫–∞",
    "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ —Å –±—ã—Å—Ç—Ä–æ–π –ø—Ä–∏–±—ã–ª—å—é",
    "–î–µ–Ω—å–≥–∏ –Ω–∞ –∫–∞—Ä—Ç—É –∑–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é",
    "–ü–∏—à–∏ –≤ –ª–∏—á–∫—É, —Ä–∞—Å—Å–∫–∞–∂—É –∫–∞–∫ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å",
    "–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –∑–∞—Ä–∞–±–æ—Ç–∫–∞",
    "–ù–∏–∫–∞–∫–∏—Ö –≤–ª–æ–∂–µ–Ω–∏–π, —Ç–æ–ª—å–∫–æ –ø—Ä–∏–±—ã–ª—å",
    "–°—Ö–µ–º–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –±–µ–∑ –æ–ø—ã—Ç–∞",
    "–†–µ—Ñ–µ—Ä–∞–ª—å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å –±–æ–Ω—É—Å–∞–º–∏",
    "–ó–∞—Ä–∞–±–∞—Ç—ã–≤–∞–π, –Ω–µ –≤—ã—Ö–æ–¥—è –∏–∑ –¥–æ–º–∞",
    "–ö—Ä–∏–ø—Ç–∞ —Ä–∞—Å—Ç–µ—Ç, —É—Å–ø–µ–π –≤–ª–æ–∂–∏—Ç—å—Å—è",
    "–°—Ç–∞–≤–∫–∏ –∏ –∫–∞–∑–∏–Ω–æ ‚Äî –±—ã—Å—Ç—Ä—ã–µ –¥–µ–Ω—å–≥–∏",
    "–¢–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è ‚Äî –∞–∫—Ü–∏—è –∏ –±–æ–Ω—É—Å",
    "–†–æ–∑—ã–≥—Ä—ã—à –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–∏–∑—ã",
]


STOP_EMBEDDINGS_EDU = [
    "–ó–∞–ø–∏—à–∏—Å—å –Ω–∞ –∫—É—Ä—Å –ø–æ –¥–∏–∑–∞–π–Ω—É",
    "–û–±—É—á–∞—é –∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥—É —Å –Ω—É–ª—è",
    "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
    "–í–µ–±–∏–Ω–∞—Ä –æ —Ç–æ–º, –∫–∞–∫ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –æ–Ω–ª–∞–π–Ω",
    "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏–Ω—Ç–µ–Ω—Å–∏–≤ –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É",
    "–ü—Ä–æ–∫–∞—á–∞–π —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ SMM",
    "–û–±—É—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥—É –∏ —Ä–µ–∫–ª–∞–º–µ",
    "–ù–æ–≤—ã–π –ø–æ—Ç–æ–∫ –∫—É—Ä—Å–∞ –ø–æ Python",
    "–û–Ω–ª–∞–π–Ω-—à–∫–æ–ª–∞ –¥–ª—è —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤",
    "–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ –∏ –º–µ–Ω—Ç–æ—Ä—Å—Ç–≤–æ",
    "–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Å—è –∫ –æ–±—É—á–∞—é—â–µ–º—É —á–∞—Ç—É",
    "–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –≤–ª–æ–∂–µ–Ω–∏–π",
]

STOP_EMBEDDINGS_MISC = [
    "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ù–æ–≤—ã–π –ø–æ—Å—Ç –Ω–∞ –∫–∞–Ω–∞–ª–µ",
    "–î–µ–ª—é—Å—å –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π",
    "–ü–æ–¥–µ–ª—é—Å—å —Å–≤–æ–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã",
    "–ù–∞—à –ø—Ä–æ–µ–∫—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è",
    "–ü–æ–∫—É–ø–∞–π—Ç–µ —É –Ω–∞—Å, —Å–∫–∏–¥–∫–∞ —Å–µ–≥–æ–¥–Ω—è",
    "–ü—Ä–∏–≥–ª–∞—à–∞—é –≤—Å–µ—Ö –Ω–∞ –º–æ–π –∫–∞–Ω–∞–ª",
    "–†–∞—Å—Å–∫–∞–∂—É, –∫–∞–∫ –Ω–∞—á–∞—Ç—å –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ–Ω–ª–∞–π–Ω",
    "–í—Å—Ç—É–ø–∞–π –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö –ª—é–¥–µ–π",
    "–°–æ–≤–µ—Ç—É—é –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –Ω–∞—à –∫–∞–Ω–∞–ª",
    "–ê–∫—Ü–∏—è —Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è",
]



STOP_EMBEDDINGS = {
    "resume": STOP_EMBEDDINGS_RESUME,
    "ads": STOP_EMBEDDINGS_ADS,
    "scam": STOP_EMBEDDINGS_SCAM,
    "edu": STOP_EMBEDDINGS_EDU,
    "misc": STOP_EMBEDDINGS_MISC,
}

def load_stop_embeddings():
    global stop_embeddings
    
    stop_embeddings = {
        cat: [model.encode(text, convert_to_tensor=True) for text in samples]
        for cat, samples in STOP_EMBEDDINGS.items()
    }


def check_stop_embeddings(text: str, threshold: float = 0.55) -> str | None:
    stop_embeddings = get_stop_embeddings()
    text_emb = model.encode(text, convert_to_tensor=True)
    for cat, emb_list in stop_embeddings.items():
        sims = [util.cos_sim(text_emb, e).item() for e in emb_list]
        if max(sims) > threshold:
            return cat
    return None


def get_stop_embeddings() -> dict[str, any]:
    return stop_embeddings


def count_stop_words(text: str) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
    –õ–æ–≥–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞.
    """
    text_lower = text.lower()
    text_words = set(re.findall(r"\b\w+\b", text_lower))  # —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    found_words = stopwords_cache.intersection(text_words)
    # print(f"Stop words cache: {stopwords_cache}")
    # print(f"Words in text: {text_words}")
    # print(f"Found stop words: {found_words}")
    return len(found_words)


async def load_professions():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à–∏:
    professions_cache –∏ professions_embeddings_cache.
    """
    global professions_cache, professions_embeddings_cache

    professions = await get_all_professions_parser()

    # –∫–µ—à —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    professions_cache = {
        p["name"]: {
            "desc": p.get("desc", ""),
            "keywords": p.get("keywords", {}),
        }
        for p in professions
    }

    # –∫–µ—à —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –æ–ø–∏—Å–∞–Ω–∏–π
    professions_embeddings_cache = {
        name: model.encode(data["desc"], convert_to_tensor=True)
        for name, data in professions_cache.items()
    }


def get_profession_embeddings() -> dict[str, any]:
    return professions_embeddings_cache


import re
import asyncio


async def contains_any_regex_async(text: str) -> bool:
    stopwords = await get_all_stopwords()
    keywords = [sw.word for sw in stopwords]

    pattern = re.compile(
        "|".join(re.escape(k.lower()) for k in keywords), re.IGNORECASE
    )

    def search():
        matches = pattern.findall(text.lower())
        for match in matches:
            logger.info(f"Found stop word: {match}")
        return bool(matches)

    return await asyncio.to_thread(search)


async def analyze_vacancy(text: str, embedding_weight: float = 0.7) -> dict:
    stop_count = await contains_any_regex_async(text)
    if stop_count:
        return {"status": "blocked", "reason": f"{stop_count} stop words found"}

    lowered = text.lower()

    # --- –æ—á–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º ---
    keyword_scores = {}
    for name, data in professions_cache.items():
        score = 0
        for kw, weight in data["keywords"].items():
            if kw.lower() in lowered:
                score += weight
        keyword_scores[name] = score
    # print(f"–û—á–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keyword_scores}")

    # --- —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º ---
    text_emb = model.encode(text, convert_to_tensor=True)
    embeddings = get_profession_embeddings()
    embedding_scores = {}
    for name, prof_emb in embeddings.items():
        sim = util.cos_sim(text_emb, prof_emb).item()
        embedding_scores[name] = sim
    # print(f"–°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {embedding_scores}")

    # --- –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ ---
    final_scores = {
        name: keyword_scores[name] + embedding_weight * embedding_scores[name]
        for name in professions_cache
    }
    # print(f"–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–π—Ç–∏–Ω–≥–∏: {final_scores}")

    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    return {"status": "ok", "ranked": ranked}


async def spam_check(text: str) -> bool:
    spam_category = check_stop_embeddings(text)
    if spam_category:
        await send_message(-4822276897, spam_category)
        await send_message(-4822276897, text)
        logger.info(f"Spam detected: {spam_category}")
        return False
    else:
        return True


# === –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ===
async def find_job_func(vacancy_text: str, embedding_weight: float = 1.1):

    result = await analyze_vacancy(vacancy_text, embedding_weight=embedding_weight)

    if result["status"] == "blocked":
        # print(f"üö´ –í–∞–∫–∞–Ω—Å–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ ({result['reason']})")
        return False

    vacancy_professions = [
        (prof, score) for prof, score in result["ranked"] if score > 1.8
    ]

    if not vacancy_professions:
        # print("‚ö†Ô∏è –í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∏–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.")
        return False

    # print("üîé –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–∏:"

    return vacancy_professions
