import asyncio
from sentence_transformers import SentenceTransformer, util
from db.requests import stopwords_cache
from db.requests import get_all_professions_parser, get_all_stopwords
from db.database import Sessionmaker
from db.models import StopWord
from sqlalchemy.future import select
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

professions_cache: dict[str, any] = {}
professions_embeddings_cache: dict[str, any] = {}
stopwords_cache: set[str] = set()

def count_stop_words(text: str) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
    –õ–æ–≥–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞.
    """
    text_lower = text.lower()
    text_words = set(re.findall(r"\b\w+\b", text_lower))  # —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    found_words = stopwords_cache.intersection(text_words)
    #print(f"Stop words cache: {stopwords_cache}")
    #print(f"Words in text: {text_words}")
    #print(f"Found stop words: {found_words}")
    return len(found_words)


async def load_professions():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à–∏:
    professions_cache –∏ professions_embeddings_cache.
    """
    global professions_cache, professions_embeddings_cache

    professions = await get_all_professions_parser()

    # –∫–µ—à —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    professions_cache = {
        p["name"]: {
            "desc": p.get("desc", ""),
            "keywords": p.get("keywords", {}),
        }
        for p in professions
    }

    # –∫–µ—à —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –æ–ø–∏—Å–∞–Ω–∏–π
    professions_embeddings_cache = {
        name: model.encode(data["desc"], convert_to_tensor=True)
        for name, data in professions_cache.items()
    }


def get_profession_embeddings() -> dict[str, any]:
    return professions_embeddings_cache



import re
import asyncio



async def contains_any_regex_async(text: str) -> bool:
    stopwords = await get_all_stopwords()
    keywords = [sw.word for sw in stopwords]
    
    pattern = re.compile("|".join(re.escape(k.lower()) for k in keywords), re.IGNORECASE)

    def search():
        matches = pattern.findall(text.lower())
        for match in matches:
            logger.info(f"Found stop word: {match}")
        return bool(matches)

    return await asyncio.to_thread(search)



async def analyze_vacancy(text: str, embedding_weight: float = 0.7) -> dict:
    stop_count = await contains_any_regex_async(text)
    if stop_count:
        return {"status": "blocked", "reason": f"{stop_count} stop words found"}

    lowered = text.lower()

    # --- –æ—á–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º ---
    keyword_scores = {}
    for name, data in professions_cache.items():
        score = 0
        for kw, weight in data["keywords"].items():
            if kw.lower() in lowered:
                score += weight
        keyword_scores[name] = score
    #print(f"–û—á–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keyword_scores}")

    # --- —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º ---
    text_emb = model.encode(text, convert_to_tensor=True)
    embeddings = get_profession_embeddings()
    embedding_scores = {}
    for name, prof_emb in embeddings.items():
        sim = util.cos_sim(text_emb, prof_emb).item()
        embedding_scores[name] = sim
    #print(f"–°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {embedding_scores}")

    # --- –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ ---
    final_scores = {
        name: keyword_scores[name] + embedding_weight * embedding_scores[name]
        for name in professions_cache
    }
    #print(f"–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–π—Ç–∏–Ω–≥–∏: {final_scores}")

    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    return {"status": "ok", "ranked": ranked}

# === –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ===
async def find_job_func(
    vacancy_text: str, embedding_weight: float = 0.7
):

    result = await analyze_vacancy(
        vacancy_text, embedding_weight=embedding_weight
    )

    if result["status"] == "blocked":
        #print(f"üö´ –í–∞–∫–∞–Ω—Å–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ ({result['reason']})")
        return False

    vacancy_professions = [
        (prof, score) for prof, score in result["ranked"] if score > 2.0
    ]

    if not vacancy_professions:
        #print("‚ö†Ô∏è –í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∏–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.")
        return False

    #print("üîé –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–∏:"

    return vacancy_professions


# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    professions = {
        "SMM-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç": {
            "desc": "–†–∞–±–æ—Ç–∞ —Å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏, –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–æ–µ–∫—Ç–æ–≤.",
            "keywords": {"smm": 2, "—Å–æ—Ü—Å–µ—Ç–∏": 1, "instagram": 1, "facebook": 1},
        },
        "–ö–æ–ø–∏—Ä–∞–π—Ç–µ—Ä": {
            "desc": "–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–∞–π—Ç–æ–≤, –±–ª–æ–≥–æ–≤, —Å–æ—Ü—Å–µ—Ç–µ–π –∏ —Ä–∞—Å—Å—ã–ª–æ–∫.",
            "keywords": {"—Ç–µ–∫—Å—Ç—ã": 2, "–∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥": 2, "—Å—Ç–∞—Ç—å–∏": 1, "–∫–æ–Ω—Ç–µ–Ω—Ç": 1},
        },
    }

    text1 = "–¢—Ä–µ–±—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ SMM –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤ Instagram"
    text2 = "–£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞: –∫–∞–∑–∏–Ω–æ, —Å—Ç–∞–≤–∫–∏ –Ω–∞ —Å–ø–æ—Ä—Ç, –∞–∑–∞—Ä—Ç–Ω—ã–µ –∏–≥—Ä—ã, –ø–∞—Ä–∏"

    asyncio.run(find_job_func(text1, professions))
    asyncio.run(find_job_func(text2, professions))


# --- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –ë–î) ---
STOP_WORDS = ["–ø–æ–º–æ–≥—É", "–∏–∑—É—á—É", "–º–µ–Ω—è –∑–æ–≤—É—Ç", "–≤–æ–∑—å–º—É –Ω–∞ —Å–µ–±—è", "–∏—â—É —Ä–∞–±–æ—Ç—É"]
